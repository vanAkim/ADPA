{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Project hypotheses\n",
    "\n",
    "## Localization\n",
    "\n",
    "Current data come from a supposedly wide audience, since Reddit is a well-known tool in the US.\n",
    "The first hypothesis states that Reddit utilisation doesn't apply a strong input filtering of the relevant population, thus biasing the dataset towards a subpopulation.\n",
    "Second hypothesis is that business context and targeted population are sufficiently similar to the dataset population, even if the location is different, such as in France where Reddit has less coverage.\n",
    "\n",
    "## Time shift\n",
    "\n",
    "Even if at current state (2022/12/26), r/RAOP rules have strong emphasis on the relative legitimacy (reddit account metadata) of applicants to avoid inappropriate requests, they cannot be taken into account here.\n",
    "Indeed, they may have evolved over time, which already adds bias to the historical data, but obviously cannot be applied retrospectively now, 9 years later.\n",
    "Nonetheless, we assume that altruism is a time constant through a wide population.\n",
    "World-wide economic situation shift over time is also neglected since our business object is a vital food product, ðŸš€ popular, and still affordable.\n",
    "\n",
    "## Wisdom of crowds\n",
    "\n",
    "Even if we disregard the rules process, Reddit structure (comments, votes, account metadata) is assimilated to an influence soft-voting tool.\n",
    "That's why it's assumed that donation process and request legitimacy are not misplaced, and we're confident about the transfert between RAOP donation purpose and our business objective.\n",
    "So if a request led to a donation, thus the request was legitimate."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Business context\n",
    "\n",
    "## Marketing campaign\n",
    "\n",
    "I'm running a pizza restaurant at a fast-growing pace with some few localizations.\n",
    "In order to promote our upcoming additional location, we're launching a marketing campaign to donate some pizza to people who made a request.\n",
    "It can leverage some pain points:\n",
    "+ Expand our brand image\n",
    "+ Minimize unsells waste\n",
    "+ Donate to people in need\n",
    "\n",
    "Currently, our resources can't afford to have dedicated people to develop and run this kind of process. Lucky for me, I used to be a Data Scientist and r/RAOP+kaggle gives me data to work with.\n",
    "\n",
    "## Business objectives\n",
    "\n",
    "1. Train a model to predict legitimacy *(i.e. pizza donation)* of a request at the moment of request to avoid target leakage.\n",
    "2. Find a process that doesn't disapprove or lower the previous legitimacy of donation at the moment of data retrieval, if there's such a thing.\n",
    "\n",
    "## Future concerns\n",
    "\n",
    "The current depicted design doesn't leverage any concerns about legitimate requests leading to actual donation and marketing performances.\n",
    "Indeed, legitimate requests could be all fulfilled or partially depending on our selection process, volume, donation supply chain, seasonality, cost efficiency, and many other variables.\n",
    "For now, the project focus on donation legitimacy modelisation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "from scipy.stats import f_oneway\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.model_selection import train_test_split, HalvingGridSearchCV\n",
    "from sklearn.feature_selection import mutual_info_classif, chi2, SelectKBest\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer, MaxAbsScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "import xgboost as xgb\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "pizza_raw_data = pd.read_json('../data/pizza_data.json',\n",
    "                              dtype={\"giver_username_if_known\": str,\n",
    "                                     \"number_of_upvotes_of_request_at_retrieval\": int,\n",
    "                                     \"post_was_edited\": bool,\n",
    "                                     \"request_id\": str,\n",
    "                                     \"request_number_of_comments_at_retrieval\": int,\n",
    "                                     \"request_text\": str,\n",
    "                                     \"request_text_edit_aware\": str,\n",
    "                                     \"request_title\": str,\n",
    "                                     \"requester_account_age_in_days_at_request\": float,\n",
    "                                     \"requester_account_age_in_days_at_retrieval\": float,\n",
    "                                     \"requester_days_since_first_post_on_raop_at_request\": float,\n",
    "                                     \"requester_days_since_first_post_on_raop_at_retrieval\": float,\n",
    "                                     \"requester_number_of_comments_at_request\": int,\n",
    "                                     \"requester_number_of_comments_at_retrieval\": int,\n",
    "                                     \"requester_number_of_comments_in_raop_at_request\": int,\n",
    "                                     \"requester_number_of_comments_in_raop_at_retrieval\": int,\n",
    "                                     \"requester_number_of_posts_at_request\": int,\n",
    "                                     \"requester_number_of_posts_at_retrieval\": int,\n",
    "                                     \"requester_number_of_posts_on_raop_at_request\": int,\n",
    "                                     \"requester_number_of_posts_on_raop_at_retrieval\": int,\n",
    "                                     \"requester_number_of_subreddits_at_request\": int,\n",
    "                                     \"requester_received_pizza\": bool,\n",
    "                                     \"requester_subreddits_at_request\": list,\n",
    "                                     \"requester_upvotes_minus_downvotes_at_request\": int,\n",
    "                                     \"requester_upvotes_minus_downvotes_at_retrieval\": int,\n",
    "                                     \"requester_upvotes_plus_downvotes_at_request\": int,\n",
    "                                     \"requester_upvotes_plus_downvotes_at_retrieval\": int,\n",
    "                                     \"requester_user_flair\": str,\n",
    "                                     \"requester_username\": str,\n",
    "                                     \"unix_timestamp_of_request\": int,\n",
    "                                     \"unix_timestamp_of_request_utc\": int})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4040 entries, 0 to 4039\n",
      "Data columns (total 32 columns):\n",
      " #   Column                                                Non-Null Count  Dtype  \n",
      "---  ------                                                --------------  -----  \n",
      " 0   giver_username_if_known                               4040 non-null   object \n",
      " 1   number_of_downvotes_of_request_at_retrieval           4040 non-null   int64  \n",
      " 2   number_of_upvotes_of_request_at_retrieval             4040 non-null   int32  \n",
      " 3   post_was_edited                                       4040 non-null   bool   \n",
      " 4   request_id                                            4040 non-null   object \n",
      " 5   request_number_of_comments_at_retrieval               4040 non-null   int32  \n",
      " 6   request_text                                          4040 non-null   object \n",
      " 7   request_text_edit_aware                               4040 non-null   object \n",
      " 8   request_title                                         4040 non-null   object \n",
      " 9   requester_account_age_in_days_at_request              4040 non-null   float64\n",
      " 10  requester_account_age_in_days_at_retrieval            4040 non-null   float64\n",
      " 11  requester_days_since_first_post_on_raop_at_request    4040 non-null   float64\n",
      " 12  requester_days_since_first_post_on_raop_at_retrieval  4040 non-null   float64\n",
      " 13  requester_number_of_comments_at_request               4040 non-null   int32  \n",
      " 14  requester_number_of_comments_at_retrieval             4040 non-null   int32  \n",
      " 15  requester_number_of_comments_in_raop_at_request       4040 non-null   int32  \n",
      " 16  requester_number_of_comments_in_raop_at_retrieval     4040 non-null   int32  \n",
      " 17  requester_number_of_posts_at_request                  4040 non-null   int32  \n",
      " 18  requester_number_of_posts_at_retrieval                4040 non-null   int32  \n",
      " 19  requester_number_of_posts_on_raop_at_request          4040 non-null   int32  \n",
      " 20  requester_number_of_posts_on_raop_at_retrieval        4040 non-null   int32  \n",
      " 21  requester_number_of_subreddits_at_request             4040 non-null   int32  \n",
      " 22  requester_received_pizza                              4040 non-null   bool   \n",
      " 23  requester_subreddits_at_request                       4040 non-null   object \n",
      " 24  requester_upvotes_minus_downvotes_at_request          4040 non-null   int32  \n",
      " 25  requester_upvotes_minus_downvotes_at_retrieval        4040 non-null   int32  \n",
      " 26  requester_upvotes_plus_downvotes_at_request           4040 non-null   int32  \n",
      " 27  requester_upvotes_plus_downvotes_at_retrieval         4040 non-null   int32  \n",
      " 28  requester_user_flair                                  4040 non-null   object \n",
      " 29  requester_username                                    4040 non-null   object \n",
      " 30  unix_timestamp_of_request                             4040 non-null   int32  \n",
      " 31  unix_timestamp_of_request_utc                         4040 non-null   int32  \n",
      "dtypes: bool(2), float64(4), int32(17), int64(1), object(8)\n",
      "memory usage: 686.6+ KB\n"
     ]
    }
   ],
   "source": [
    "pizza_raw_data.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataset has no missing values, so imputation processes will not be covered in this notebook, but should be considered in a full production-ready pipeline."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data leakage prevention\n",
    "\n",
    "Some features may lead to data leakage.\n",
    "One is directly linked to pizza donation, `giver_username_if_known`.\n",
    "Others may be since they aren't concerned about at_request/at_retrieval split-up, such as `requester_user_flair` *(requester badge obtention after receiving a pizza donation)*, `request_text` and `post_was_edited` *(some request posts are edited after getting a pizza donation)*.\n",
    "So, these features are removed from our project."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "pizza_prevented_data = pizza_raw_data.loc[:, ~(pizza_raw_data\n",
    "                                               .columns\n",
    "                                               .isin([\"giver_username_if_known\",\n",
    "                                                      \"requester_user_flair\",\n",
    "                                                      \"request_text\",\n",
    "                                                      \"post_was_edited\"]))\n",
    "                       ]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split training data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "target_name = 'requester_received_pizza'\n",
    "seed = 101\n",
    "X = pizza_prevented_data.copy()\n",
    "y = X.pop(target_name)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=seed, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=seed, stratify=y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From our 4040 samples, we'll use 3636 of them to train the model.\n",
      "From 3636 requests, 895 led to a donation, which represents 24.6%.\n",
      "Thus, among 1026 days, 1 pizza was donate every 1.15 day.\n"
     ]
    }
   ],
   "source": [
    "dataset_period = (dt.strptime(\"29/09/2013\", \"%d/%m/%Y\") - dt.strptime(\"08/12/2010\", \"%d/%m/%Y\")).days\n",
    "\n",
    "print(f'''From our {pizza_raw_data.shape[0]} samples, we'll use {X_train.shape[0]} of them to train the model.\n",
    "From {len(y_train)} requests, {y_train.sum()} led to a donation, which represents {round(y_train.sum()/len(y_train), 3)*100}%.\n",
    "Thus, among {dataset_period} days, 1 pizza was donate every {round(dataset_period/y_train.sum(), 2)} day.''')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "One request out of four is therefore legitimate according to our statements."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dissociate features at request from at retrieval\n",
    "\n",
    "In order to avoid data leakage, for example a request that had a donation could have a posteriori some upvotes boost, only features at request time are accounted according to the first objective to model legitimacy of a request."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "univariate_features = [\"request_id\",\n",
    "                       \"requester_username\",\n",
    "                       \"unix_timestamp_of_request_utc\", #non-utc timestamp is redundant and less convenient\n",
    "                       \"request_title\",\n",
    "                       \"request_text_edit_aware\"]\n",
    "\n",
    "at_request_features = []\n",
    "at_retrieval_features = []\n",
    "\n",
    "for selected_time, selected_features in {\"at_request\": at_request_features, \"at_retrieval\": at_retrieval_features}.items():\n",
    "    dataset_features = (X_train\n",
    "                        .filter(regex=f'.*{selected_time}$')\n",
    "                        .columns\n",
    "                        .tolist())\n",
    "    selected_features.extend(dataset_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "X_train = X_train[univariate_features + at_request_features]\n",
    "#pizza_retrieval_data = X_train[univariate_features + at_retrieval_features]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 raw features can be used to predict legitimacy of a request.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{X_train.shape[1]} raw features can be used to predict legitimacy of a request.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "       request_id requester_username  unix_timestamp_of_request_utc  \\\ncount        3636               3636                   3.636000e+03   \nunique       3636               3636                            NaN   \ntop     t3_101kt0      ColoredPencil                            NaN   \nfreq            1                  1                            NaN   \nmean          NaN                NaN                   1.342677e+09   \nstd           NaN                NaN                   2.332738e+07   \nmin           NaN                NaN                   1.297723e+09   \n25%           NaN                NaN                   1.320252e+09   \n50%           NaN                NaN                   1.342484e+09   \n75%           NaN                NaN                   1.364254e+09   \nmax           NaN                NaN                   1.381523e+09   \n\n       request_title request_text_edit_aware  \\\ncount           3636                    3636   \nunique          3625                    3538   \ntop          Request                           \nfreq               4                      95   \nmean             NaN                     NaN   \nstd              NaN                     NaN   \nmin              NaN                     NaN   \n25%              NaN                     NaN   \n50%              NaN                     NaN   \n75%              NaN                     NaN   \nmax              NaN                     NaN   \n\n        requester_account_age_in_days_at_request  \\\ncount                                3636.000000   \nunique                                       NaN   \ntop                                          NaN   \nfreq                                         NaN   \nmean                                  251.623667   \nstd                                   297.644679   \nmin                                     0.000000   \n25%                                     2.872529   \n50%                                   156.997483   \n75%                                   386.092488   \nmax                                  2521.263206   \n\n        requester_days_since_first_post_on_raop_at_request  \\\ncount                                         3636.000000    \nunique                                                NaN    \ntop                                                   NaN    \nfreq                                                  NaN    \nmean                                            15.827121    \nstd                                             67.651509    \nmin                                              0.000000    \n25%                                              0.000000    \n50%                                              0.000000    \n75%                                              0.000000    \nmax                                            785.457685    \n\n        requester_number_of_comments_at_request  \\\ncount                               3636.000000   \nunique                                      NaN   \ntop                                         NaN   \nfreq                                        NaN   \nmean                                 115.032178   \nstd                                  193.458792   \nmin                                    0.000000   \n25%                                    0.000000   \n50%                                   24.000000   \n75%                                  139.000000   \nmax                                  992.000000   \n\n        requester_number_of_comments_in_raop_at_request  \\\ncount                                       3636.000000   \nunique                                              NaN   \ntop                                                 NaN   \nfreq                                                NaN   \nmean                                           0.646040   \nstd                                            3.323355   \nmin                                            0.000000   \n25%                                            0.000000   \n50%                                            0.000000   \n75%                                            0.000000   \nmax                                           88.000000   \n\n        requester_number_of_posts_at_request  \\\ncount                            3636.000000   \nunique                                   NaN   \ntop                                      NaN   \nfreq                                     NaN   \nmean                               21.363311   \nstd                                50.180486   \nmin                                 0.000000   \n25%                                 0.000000   \n50%                                 5.000000   \n75%                                22.000000   \nmax                               867.000000   \n\n        requester_number_of_posts_on_raop_at_request  \\\ncount                                    3636.000000   \nunique                                           NaN   \ntop                                              NaN   \nfreq                                             NaN   \nmean                                        0.065457   \nstd                                         0.329403   \nmin                                         0.000000   \n25%                                         0.000000   \n50%                                         0.000000   \n75%                                         0.000000   \nmax                                         5.000000   \n\n        requester_number_of_subreddits_at_request  \\\ncount                                 3636.000000   \nunique                                        NaN   \ntop                                           NaN   \nfreq                                          NaN   \nmean                                    18.003850   \nstd                                     21.648384   \nmin                                      0.000000   \n25%                                      1.000000   \n50%                                     11.000000   \n75%                                     27.000000   \nmax                                    186.000000   \n\n       requester_subreddits_at_request  \\\ncount                             3636   \nunique                            2683   \ntop                                 []   \nfreq                               667   \nmean                               NaN   \nstd                                NaN   \nmin                                NaN   \n25%                                NaN   \n50%                                NaN   \n75%                                NaN   \nmax                                NaN   \n\n        requester_upvotes_minus_downvotes_at_request  \\\ncount                                    3636.000000   \nunique                                           NaN   \ntop                                              NaN   \nfreq                                             NaN   \nmean                                     1163.397690   \nstd                                      3853.827389   \nmin                                      -173.000000   \n25%                                         3.000000   \n50%                                       173.500000   \n75%                                      1153.000000   \nmax                                    155010.000000   \n\n        requester_upvotes_plus_downvotes_at_request  \ncount                                  3.636000e+03  \nunique                                          NaN  \ntop                                             NaN  \nfreq                                            NaN  \nmean                                   3.807502e+03  \nstd                                    2.707045e+04  \nmin                                    0.000000e+00  \n25%                                    8.000000e+00  \n50%                                    3.505000e+02  \n75%                                    2.290250e+03  \nmax                                    1.286864e+06  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>request_id</th>\n      <th>requester_username</th>\n      <th>unix_timestamp_of_request_utc</th>\n      <th>request_title</th>\n      <th>request_text_edit_aware</th>\n      <th>requester_account_age_in_days_at_request</th>\n      <th>requester_days_since_first_post_on_raop_at_request</th>\n      <th>requester_number_of_comments_at_request</th>\n      <th>requester_number_of_comments_in_raop_at_request</th>\n      <th>requester_number_of_posts_at_request</th>\n      <th>requester_number_of_posts_on_raop_at_request</th>\n      <th>requester_number_of_subreddits_at_request</th>\n      <th>requester_subreddits_at_request</th>\n      <th>requester_upvotes_minus_downvotes_at_request</th>\n      <th>requester_upvotes_plus_downvotes_at_request</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>3636</td>\n      <td>3636</td>\n      <td>3.636000e+03</td>\n      <td>3636</td>\n      <td>3636</td>\n      <td>3636.000000</td>\n      <td>3636.000000</td>\n      <td>3636.000000</td>\n      <td>3636.000000</td>\n      <td>3636.000000</td>\n      <td>3636.000000</td>\n      <td>3636.000000</td>\n      <td>3636</td>\n      <td>3636.000000</td>\n      <td>3.636000e+03</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>3636</td>\n      <td>3636</td>\n      <td>NaN</td>\n      <td>3625</td>\n      <td>3538</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2683</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>t3_101kt0</td>\n      <td>ColoredPencil</td>\n      <td>NaN</td>\n      <td>Request</td>\n      <td></td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[]</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>1</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>95</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>667</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.342677e+09</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>251.623667</td>\n      <td>15.827121</td>\n      <td>115.032178</td>\n      <td>0.646040</td>\n      <td>21.363311</td>\n      <td>0.065457</td>\n      <td>18.003850</td>\n      <td>NaN</td>\n      <td>1163.397690</td>\n      <td>3.807502e+03</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.332738e+07</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>297.644679</td>\n      <td>67.651509</td>\n      <td>193.458792</td>\n      <td>3.323355</td>\n      <td>50.180486</td>\n      <td>0.329403</td>\n      <td>21.648384</td>\n      <td>NaN</td>\n      <td>3853.827389</td>\n      <td>2.707045e+04</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.297723e+09</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>-173.000000</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.320252e+09</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.872529</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>3.000000</td>\n      <td>8.000000e+00</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.342484e+09</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>156.997483</td>\n      <td>0.000000</td>\n      <td>24.000000</td>\n      <td>0.000000</td>\n      <td>5.000000</td>\n      <td>0.000000</td>\n      <td>11.000000</td>\n      <td>NaN</td>\n      <td>173.500000</td>\n      <td>3.505000e+02</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.364254e+09</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>386.092488</td>\n      <td>0.000000</td>\n      <td>139.000000</td>\n      <td>0.000000</td>\n      <td>22.000000</td>\n      <td>0.000000</td>\n      <td>27.000000</td>\n      <td>NaN</td>\n      <td>1153.000000</td>\n      <td>2.290250e+03</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.381523e+09</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2521.263206</td>\n      <td>785.457685</td>\n      <td>992.000000</td>\n      <td>88.000000</td>\n      <td>867.000000</td>\n      <td>5.000000</td>\n      <td>186.000000</td>\n      <td>NaN</td>\n      <td>155010.000000</td>\n      <td>1.286864e+06</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe(include=\"all\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data exploration\n",
    "## Let's start first with non-textual data ...\n",
    "\n",
    "In order to have ground level refrence, let's start with a very basic modelisation with underperforming results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "X_train_num = X_train.select_dtypes(exclude=[\"object\"])\n",
    "X_val_num = X_val.select_dtypes(exclude=[\"object\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3636 entries, 1186 to 813\n",
      "Data columns (total 10 columns):\n",
      " #   Column                                              Non-Null Count  Dtype  \n",
      "---  ------                                              --------------  -----  \n",
      " 0   unix_timestamp_of_request_utc                       3636 non-null   int32  \n",
      " 1   requester_account_age_in_days_at_request            3636 non-null   float64\n",
      " 2   requester_days_since_first_post_on_raop_at_request  3636 non-null   float64\n",
      " 3   requester_number_of_comments_at_request             3636 non-null   int32  \n",
      " 4   requester_number_of_comments_in_raop_at_request     3636 non-null   int32  \n",
      " 5   requester_number_of_posts_at_request                3636 non-null   int32  \n",
      " 6   requester_number_of_posts_on_raop_at_request        3636 non-null   int32  \n",
      " 7   requester_number_of_subreddits_at_request           3636 non-null   int32  \n",
      " 8   requester_upvotes_minus_downvotes_at_request        3636 non-null   int32  \n",
      " 9   requester_upvotes_plus_downvotes_at_request         3636 non-null   int32  \n",
      "dtypes: float64(2), int32(8)\n",
      "memory usage: 198.8 KB\n"
     ]
    }
   ],
   "source": [
    "X_train_num.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature engineering\n",
    "\n",
    "Votes give great insights with the sum related to visibility and difference related to consensus/polarisation. However, the 2 variables here are absolute and a relative one is missing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def add_relative_votes(df):\n",
    "    num = 'requester_upvotes_minus_downvotes_at_request'\n",
    "    den = 'requester_upvotes_plus_downvotes_at_request'\n",
    "    math_fct = lambda row: row.iloc[1] and row.iloc[0] / row.iloc[1] or 0\n",
    "    df['requester_relative_consensual_votes_at_request'] = df.copy()[[num, den]].apply(math_fct, axis = 1)\n",
    "\n",
    "    return df\n",
    "\n",
    "X_train_num = add_relative_votes(X_train_num)\n",
    "X_val_num = add_relative_votes(X_val_num)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature importances"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def make_mi_scores(x_data, y_data):\n",
    "    mi_results = mutual_info_classif(x_data, y_data, random_state=seed)\n",
    "    mi_results = pd.Series(mi_results, name=\"MI Scores\", index=x_data.columns)\n",
    "    return mi_results\n",
    "\n",
    "def make_f_oneway_scores(x_data, y_data):\n",
    "    grp_anova = x_data.groupby(y_data)\n",
    "\n",
    "    f_values = []\n",
    "    for feat in x_data.columns:\n",
    "        s, p = f_oneway(grp_anova.get_group(0)[feat],\n",
    "                        grp_anova.get_group(1)[feat])\n",
    "        s = round(s, 4)\n",
    "        p = round(p, 4)\n",
    "        f_values.append((s, p))\n",
    "\n",
    "    f_scores = pd.Series(f_values, name=\"F_oneway Scores\", index=x_data.columns)\n",
    "    return f_scores\n",
    "\n",
    "def make_corr_scores(x_data, y_data):\n",
    "    return x_data.corrwith(y_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                    Mutual Info  Correlation  \\\nunix_timestamp_of_request_utc                          0.024898    -0.107105   \nrequester_account_age_in_days_at_request               0.000000     0.043291   \nrequester_days_since_first_post_on_raop_at_request     0.002130     0.101339   \nrequester_number_of_comments_at_request                0.000000     0.034051   \nrequester_number_of_comments_in_raop_at_request        0.013583     0.131384   \nrequester_number_of_posts_at_request                   0.006105     0.014886   \nrequester_number_of_posts_on_raop_at_request           0.002615     0.138441   \nrequester_number_of_subreddits_at_request              0.000011     0.038775   \nrequester_upvotes_minus_downvotes_at_request           0.010255     0.031172   \nrequester_upvotes_plus_downvotes_at_request            0.000000     0.025642   \nrequester_relative_consensual_votes_at_request         0.000000     0.080072   \n\n                                                      F_oneway (s,p)  \nunix_timestamp_of_request_utc                         (42.1707, 0.0)  \nrequester_account_age_in_days_at_request             (6.8232, 0.009)  \nrequester_days_since_first_post_on_raop_at_request    (37.7071, 0.0)  \nrequester_number_of_comments_at_request             (4.2183, 0.0401)  \nrequester_number_of_comments_in_raop_at_request       (63.8311, 0.0)  \nrequester_number_of_posts_at_request                (0.8055, 0.3695)  \nrequester_number_of_posts_on_raop_at_request          (71.0095, 0.0)  \nrequester_number_of_subreddits_at_request           (5.4718, 0.0194)  \nrequester_upvotes_minus_downvotes_at_request        (3.5346, 0.0602)  \nrequester_upvotes_plus_downvotes_at_request         (2.3909, 0.1221)  \nrequester_relative_consensual_votes_at_request          (23.45, 0.0)  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Mutual Info</th>\n      <th>Correlation</th>\n      <th>F_oneway (s,p)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>unix_timestamp_of_request_utc</th>\n      <td>0.024898</td>\n      <td>-0.107105</td>\n      <td>(42.1707, 0.0)</td>\n    </tr>\n    <tr>\n      <th>requester_account_age_in_days_at_request</th>\n      <td>0.000000</td>\n      <td>0.043291</td>\n      <td>(6.8232, 0.009)</td>\n    </tr>\n    <tr>\n      <th>requester_days_since_first_post_on_raop_at_request</th>\n      <td>0.002130</td>\n      <td>0.101339</td>\n      <td>(37.7071, 0.0)</td>\n    </tr>\n    <tr>\n      <th>requester_number_of_comments_at_request</th>\n      <td>0.000000</td>\n      <td>0.034051</td>\n      <td>(4.2183, 0.0401)</td>\n    </tr>\n    <tr>\n      <th>requester_number_of_comments_in_raop_at_request</th>\n      <td>0.013583</td>\n      <td>0.131384</td>\n      <td>(63.8311, 0.0)</td>\n    </tr>\n    <tr>\n      <th>requester_number_of_posts_at_request</th>\n      <td>0.006105</td>\n      <td>0.014886</td>\n      <td>(0.8055, 0.3695)</td>\n    </tr>\n    <tr>\n      <th>requester_number_of_posts_on_raop_at_request</th>\n      <td>0.002615</td>\n      <td>0.138441</td>\n      <td>(71.0095, 0.0)</td>\n    </tr>\n    <tr>\n      <th>requester_number_of_subreddits_at_request</th>\n      <td>0.000011</td>\n      <td>0.038775</td>\n      <td>(5.4718, 0.0194)</td>\n    </tr>\n    <tr>\n      <th>requester_upvotes_minus_downvotes_at_request</th>\n      <td>0.010255</td>\n      <td>0.031172</td>\n      <td>(3.5346, 0.0602)</td>\n    </tr>\n    <tr>\n      <th>requester_upvotes_plus_downvotes_at_request</th>\n      <td>0.000000</td>\n      <td>0.025642</td>\n      <td>(2.3909, 0.1221)</td>\n    </tr>\n    <tr>\n      <th>requester_relative_consensual_votes_at_request</th>\n      <td>0.000000</td>\n      <td>0.080072</td>\n      <td>(23.45, 0.0)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mi_scores = make_mi_scores(X_train_num, y_train)\n",
    "f_oneway_scores = make_f_oneway_scores(X_train_num, y_train)\n",
    "corr_scores = make_corr_scores(X_train_num, y_train)\n",
    "\n",
    "pd_info = pd.concat([mi_scores, corr_scores, f_oneway_scores], axis='columns')\n",
    "pd_info.columns =[\"Mutual Info\", \"Correlation\", \"F_oneway (s,p)\"]\n",
    "pd_info"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It seems that almost all features have some importance, with `requester_number_of_posts_at_request` being the least important of them.\n",
    "The previous created `requester_relative_consensual_votes_at_request` feature looks like a good new addition, and beating its creation features."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modelisation\n",
    "\n",
    "Now that we're at the modelisation step, there is one important question, which is what metric to use.\n",
    "The real subquestion is, in terms of business, what is more prolific and harmless in the fact of donate to a mostly illegitimate request or on contrary not donating to a legitimate request.\n",
    "The first, a false positive, harms the branding image and financial cost, the other, a false negative, harms notoriety.\n",
    "But most of all, as described earlier, this algorithm doesn't lead directly to donation. It outputs only so-called legitimacy, which will take a big place in the donation decision but still can be hold back on a decision algorithm.\n",
    "With that in mind, the main goal should be to minimize false positives with a metric such as precision, as long as false negatives are not numerous."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def create_scaler_preproc(numerical_features, scaler_method = None):\n",
    "    steps = [('scaler_method', scaler_method)]\n",
    "    scaler_transformer = Pipeline(steps)\n",
    "\n",
    "    # Preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('scaler_transformer', scaler_transformer, numerical_features)\n",
    "        ], remainder='drop')\n",
    "\n",
    "    # Model pipeline\n",
    "    steps = [('preproc', preprocessor)]\n",
    "    proc_pipe = Pipeline(steps)\n",
    "\n",
    "    return proc_pipe\n",
    "\n",
    "def create_pipeline(train_set, preproc, estimator, model_name, grid_strat, hyperparameters, n_folds,\n",
    "                    eval_metric='precision', verbosity=3, n_jobs=10):\n",
    "    X, y = train_set\n",
    "\n",
    "    # Model pipeline\n",
    "    steps = [(\"preproc\", preproc), (model_name, estimator)]\n",
    "    model_pipe = Pipeline(steps)\n",
    "\n",
    "    # Grid Search\n",
    "    cv = grid_strat(model_pipe,\n",
    "                    param_grid=hyperparameters,\n",
    "                    cv=n_folds,\n",
    "                    scoring=eval_metric,\n",
    "                    n_jobs=n_jobs,\n",
    "                    verbose=verbosity,\n",
    "                    random_state=seed)\n",
    "\n",
    "    grid_model = cv.fit(X, y)\n",
    "\n",
    "    return grid_model\n",
    "\n",
    "def print_pipe_results(train_set, val_set, model):\n",
    "    X_train, y_train = train_set\n",
    "    yhat = model.best_estimator_.predict(X_train)\n",
    "    print(f'In samples resutls:\\n {classification_report(y_train, yhat)}')\n",
    "    print()\n",
    "\n",
    "    X_val, y_val = val_set\n",
    "    yhat = model.best_estimator_.predict(X_val)\n",
    "    print(f'Out-of-samples resutls:\\n {classification_report(y_val, yhat)}')\n",
    "    print()\n",
    "    print(f'''Confusion matrix:\n",
    "{pd.DataFrame(confusion_matrix(y_val, yhat, normalize='all'),\n",
    "              columns=[\"PredNeg\", \"PredPos\"],\n",
    "              index=[\"Neg\", \"Pos\"])}''')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In samples resutls:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.76      0.99      0.86      2741\n",
      "        True       0.70      0.05      0.09       895\n",
      "\n",
      "    accuracy                           0.76      3636\n",
      "   macro avg       0.73      0.52      0.48      3636\n",
      "weighted avg       0.75      0.76      0.67      3636\n",
      "\n",
      "\n",
      "Out-of-samples resutls:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.77      1.00      0.87       153\n",
      "        True       1.00      0.06      0.12        49\n",
      "\n",
      "    accuracy                           0.77       202\n",
      "   macro avg       0.88      0.53      0.49       202\n",
      "weighted avg       0.82      0.77      0.69       202\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "      PredNeg   PredPos\n",
      "Neg  0.757426  0.000000\n",
      "Pos  0.227723  0.014851\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(solver='liblinear',\n",
    "                           random_state=seed)\n",
    "\n",
    "model_name = 'logreg'\n",
    "\n",
    "parameters = {f'{model_name}__C': np.logspace(-2, 1, 50)}\n",
    "\n",
    "logreg_gridCV = create_pipeline(train_set = [X_train_num, y_train],\n",
    "                                preproc=create_scaler_preproc(X_train_num.columns, scaler_method=MaxAbsScaler()),\n",
    "                                estimator = model,\n",
    "                                model_name = model_name,\n",
    "                                grid_strat=HalvingGridSearchCV,\n",
    "                                hyperparameters = parameters,\n",
    "                                n_folds = 4,\n",
    "                                eval_metric='precision',\n",
    "                                verbosity=0)\n",
    "\n",
    "print()\n",
    "print_pipe_results(train_set = [X_train_num, y_train],\n",
    "                   val_set = [X_val_num, y_val],\n",
    "                   model = logreg_gridCV)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Logistic regression doesn't offer great recall performance whatever the tuning."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Gaussian Naive Bayes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In samples resutls:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.77      0.93      0.84      2741\n",
      "        True       0.41      0.14      0.21       895\n",
      "\n",
      "    accuracy                           0.74      3636\n",
      "   macro avg       0.59      0.54      0.53      3636\n",
      "weighted avg       0.68      0.74      0.69      3636\n",
      "\n",
      "\n",
      "Out-of-samples resutls:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.76      0.93      0.84       153\n",
      "        True       0.33      0.10      0.16        49\n",
      "\n",
      "    accuracy                           0.73       202\n",
      "   macro avg       0.55      0.52      0.50       202\n",
      "weighted avg       0.66      0.73      0.68       202\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "      PredNeg   PredPos\n",
      "Neg  0.707921  0.049505\n",
      "Pos  0.217822  0.024752\n"
     ]
    }
   ],
   "source": [
    "model = GaussianNB()\n",
    "\n",
    "model_name = 'gaus_nb'\n",
    "\n",
    "parameters = {}\n",
    "\n",
    "gaussnb_gridCV = create_pipeline(train_set = [X_train_num, y_train],\n",
    "                                 preproc=create_scaler_preproc(X_train_num.columns, scaler_method=StandardScaler()),\n",
    "                                 estimator = model,\n",
    "                                 model_name = model_name,\n",
    "                                 grid_strat=HalvingGridSearchCV,\n",
    "                                 hyperparameters = parameters,\n",
    "                                 n_folds = 4,\n",
    "                                 eval_metric='precision',\n",
    "                                 verbosity=0)\n",
    "\n",
    "print()\n",
    "print_pipe_results(train_set = [X_train_num, y_train],\n",
    "                   val_set = [X_val_num, y_val],\n",
    "                   model = gaussnb_gridCV)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Random Forest Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In samples resutls:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.79      1.00      0.88      2741\n",
      "        True       0.94      0.18      0.30       895\n",
      "\n",
      "    accuracy                           0.79      3636\n",
      "   macro avg       0.86      0.59      0.59      3636\n",
      "weighted avg       0.83      0.79      0.74      3636\n",
      "\n",
      "\n",
      "Out-of-samples resutls:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.77      0.97      0.86       153\n",
      "        True       0.56      0.10      0.17        49\n",
      "\n",
      "    accuracy                           0.76       202\n",
      "   macro avg       0.66      0.54      0.52       202\n",
      "weighted avg       0.72      0.76      0.69       202\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "      PredNeg   PredPos\n",
      "Neg  0.737624  0.019802\n",
      "Pos  0.217822  0.024752\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=seed, n_jobs=10)\n",
    "\n",
    "model_name = 'rf'\n",
    "\n",
    "parameters = {f'{model_name}__n_estimators': np.linspace(8, 100, 8).astype(int),\n",
    "              f'{model_name}__max_depth': np.linspace(2, 8, 8).astype(int)}\n",
    "\n",
    "rf_gridCV = create_pipeline(train_set = [X_train_num, y_train],\n",
    "                            preproc=create_scaler_preproc(X_train_num.columns, scaler_method=None),\n",
    "                            estimator = model,\n",
    "                            model_name = model_name,\n",
    "                            grid_strat=HalvingGridSearchCV,\n",
    "                            hyperparameters = parameters,\n",
    "                            n_folds = 4,\n",
    "                            eval_metric='f1', #better overall, little downgrade on precision high boost on recall\n",
    "                            verbosity=0)\n",
    "\n",
    "print()\n",
    "print_pipe_results(train_set = [X_train_num, y_train],\n",
    "                   val_set = [X_val_num, y_val],\n",
    "                   model = rf_gridCV)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### XGBoost Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 4\n",
      "n_required_iterations: 4\n",
      "n_possible_iterations: 4\n",
      "min_resources_: 134\n",
      "max_resources_: 3636\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 48\n",
      "n_resources: 134\n",
      "Fitting 4 folds for each of 48 candidates, totalling 192 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 16\n",
      "n_resources: 402\n",
      "Fitting 4 folds for each of 16 candidates, totalling 64 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 6\n",
      "n_resources: 1206\n",
      "Fitting 4 folds for each of 6 candidates, totalling 24 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 2\n",
      "n_resources: 3618\n",
      "Fitting 4 folds for each of 2 candidates, totalling 8 fits\n",
      "\n",
      "In samples resutls:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.77      0.99      0.87      2741\n",
      "        True       0.75      0.11      0.20       895\n",
      "\n",
      "    accuracy                           0.77      3636\n",
      "   macro avg       0.76      0.55      0.53      3636\n",
      "weighted avg       0.77      0.77      0.70      3636\n",
      "\n",
      "\n",
      "Out-of-samples resutls:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.78      0.98      0.87       153\n",
      "        True       0.67      0.12      0.21        49\n",
      "\n",
      "    accuracy                           0.77       202\n",
      "   macro avg       0.72      0.55      0.54       202\n",
      "weighted avg       0.75      0.77      0.71       202\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "      PredNeg   PredPos\n",
      "Neg  0.742574  0.014851\n",
      "Pos  0.212871  0.029703\n"
     ]
    }
   ],
   "source": [
    "model = xgb.XGBClassifier(random_state=seed,\n",
    "                          objective='binary:logistic',\n",
    "                          eval_metric=precision_score,\n",
    "                          tree_method='gpu_hist')\n",
    "\n",
    "model_name = 'xgb_cl'\n",
    "\n",
    "parameters = {f'{model_name}__n_estimators': np.linspace(30, 60, 4).astype(int),\n",
    "              f'{model_name}__learning_rate': np.logspace(-1.5, -0.5, 4),\n",
    "              f'{model_name}__max_depth': np.linspace(2, 7, 3).astype(int),\n",
    "              # f'{model_name}__colsample_bytree': np.logspace(-0.7, 0, 2),\n",
    "              # f'{model_name}__subsample': np.logspace(-0.7, 0, 2)\n",
    "             }\n",
    "\n",
    "xgb_gridCV = create_pipeline(train_set = [X_train_num, y_train],\n",
    "                             preproc=create_scaler_preproc(X_train_num.columns, scaler_method=None),\n",
    "                             estimator = model,\n",
    "                             model_name = model_name,\n",
    "                             grid_strat=HalvingGridSearchCV,\n",
    "                             hyperparameters = parameters,\n",
    "                             n_folds = 4,\n",
    "                             eval_metric='precision', # f1 is bad -> 0.47 precision / 0.46 recall\n",
    "                             verbosity=1)\n",
    "\n",
    "print()\n",
    "print_pipe_results(train_set = [X_train_num, y_train],\n",
    "                   val_set = [X_val_num, y_val],\n",
    "                   model = xgb_gridCV)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Conclusion\n",
    "\n",
    "After model comparison and light hypertunings, results aren't very conclusive. It was expected with only these variables *(backed up by the features importance part)*.\n",
    "It seems that predicting precisely positive class is quite challenging with the current process.\n",
    "Let's move to textual modelisation to pump up our game."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ... and get the final word."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "X_train_text = X_train.select_dtypes(include=[\"object\"])\n",
    "X_val_text = X_val.select_dtypes(include=[\"object\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3636 entries, 1186 to 813\n",
      "Data columns (total 5 columns):\n",
      " #   Column                           Non-Null Count  Dtype \n",
      "---  ------                           --------------  ----- \n",
      " 0   request_id                       3636 non-null   object\n",
      " 1   requester_username               3636 non-null   object\n",
      " 2   request_title                    3636 non-null   object\n",
      " 3   request_text_edit_aware          3636 non-null   object\n",
      " 4   requester_subreddits_at_request  3636 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 170.4+ KB\n"
     ]
    }
   ],
   "source": [
    "X_train_text.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Only `request_title` and `request_text_edit_aware` are NLP oriented features, so it'll be the main focus."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Cleaning\n",
    "\n",
    "In order to process textual data, a data cleaning process is required, in order to standardize case sensitivity and too little or not enough word occurrences.\n",
    "On a second step, we can try different process to search for improvements, such as stemming/lemmanization and other techniques.\n",
    "\n",
    "To get there, we can use a useful function of scikit-learn that handles tokenization, stop words removal, and BOW process."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "stopwords_en = set(stopwords.words('english')).union(set(string.punctuation))\n",
    "stopwords_en = stopwords_en.union(word_tokenize(' '.join(stopwords_en)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modelisation\n",
    "\n",
    "Like non-textual data, we can repeat the modelisation step and get some comparisons."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "outputs": [],
   "source": [
    "def create_nlp_preproc(post_feature, title_feature, reducter=None, scaler=None):\n",
    "    # Word vectorizers\n",
    "    # Post data\n",
    "    text_vectorizer = CountVectorizer(lowercase=True,\n",
    "                                      tokenizer=word_tokenize,\n",
    "                                      token_pattern=None,\n",
    "                                      stop_words=list(stopwords_en),\n",
    "                                      min_df=3,\n",
    "                                      ngram_range=(1,2))\n",
    "    text_tfidf = TfidfTransformer()\n",
    "    text_reduc = reducter\n",
    "    text_scaler = scaler\n",
    "    steps = [('text_count', text_vectorizer), ('text_tfidf', text_tfidf), ('text_reduc', text_reduc), ('text_scaler', text_scaler)]\n",
    "    text_pipe = Pipeline(steps)\n",
    "\n",
    "    # Preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('wordvec_post_transformer', text_pipe, post_feature),\n",
    "            ('wordvec_title_transformer', text_pipe, title_feature)\n",
    "        ], remainder='drop')\n",
    "\n",
    "    steps = [('preproc', preprocessor)]\n",
    "    preproc_pipe = Pipeline(steps)\n",
    "\n",
    "    return preproc_pipe\n",
    "\n",
    "def create_nlp_pipeline(train_set, preproc, estimator, model_name, grid_strat, hyperparameters, n_folds,\n",
    "                        eval_metric='precision', verbosity=1, n_jobs=10):\n",
    "    X, y = train_set\n",
    "\n",
    "    # Model pipeline\n",
    "    steps = [(\"preproc\", preproc), (model_name, estimator)]\n",
    "    model_pipe = Pipeline(steps)\n",
    "\n",
    "    # Grid Search\n",
    "    cv = grid_strat(model_pipe,\n",
    "                    param_grid=hyperparameters,\n",
    "                    cv=n_folds,\n",
    "                    scoring=eval_metric,\n",
    "                    n_jobs=n_jobs,\n",
    "                    verbose=verbosity,\n",
    "                    random_state=seed)\n",
    "\n",
    "    grid_model = cv.fit(X, y)\n",
    "\n",
    "    return grid_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HalvingGridSearchCV' object has no attribute 'fit_transform'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[239], line 8\u001B[0m\n\u001B[0;32m      4\u001B[0m model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlogreg_nlp\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      6\u001B[0m parameters \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m__C\u001B[39m\u001B[38;5;124m'\u001B[39m: np\u001B[38;5;241m.\u001B[39mlogspace(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m15\u001B[39m)}\n\u001B[1;32m----> 8\u001B[0m logreg_nlp_gridCV \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_nlp_pipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_set\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mX_train_text\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m                                        \u001B[49m\u001B[43mpreproc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcreate_nlp_preproc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpost_feature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrequest_text_edit_aware\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m                                                                   \u001B[49m\u001B[43mtitle_feature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrequest_title\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m                                                                   \u001B[49m\u001B[43mscaler\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mNormalizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m                                        \u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[43m                                        \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[43m                                        \u001B[49m\u001B[43mgrid_strat\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mHalvingGridSearchCV\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[43m                                        \u001B[49m\u001B[43mhyperparameters\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[43m                                        \u001B[49m\u001B[43mn_folds\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[43m                                        \u001B[49m\u001B[43meval_metric\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mprecision\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     18\u001B[0m \u001B[43m                                        \u001B[49m\u001B[43mverbosity\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28mprint\u001B[39m()\n\u001B[0;32m     21\u001B[0m print_pipe_results(train_set \u001B[38;5;241m=\u001B[39m [X_train_text, y_train],\n\u001B[0;32m     22\u001B[0m                    val_set \u001B[38;5;241m=\u001B[39m [X_val_text, y_val],\n\u001B[0;32m     23\u001B[0m                    model \u001B[38;5;241m=\u001B[39m logreg_nlp_gridCV)\n",
      "Cell \u001B[1;32mIn[238], line 45\u001B[0m, in \u001B[0;36mcreate_nlp_pipeline\u001B[1;34m(train_set, preproc, estimator, model_name, grid_strat, hyperparameters, n_folds, eval_metric, verbosity, n_jobs)\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;66;03m# Grid Search\u001B[39;00m\n\u001B[0;32m     37\u001B[0m cv \u001B[38;5;241m=\u001B[39m grid_strat(model_pipe,\n\u001B[0;32m     38\u001B[0m                 param_grid\u001B[38;5;241m=\u001B[39mhyperparameters,\n\u001B[0;32m     39\u001B[0m                 cv\u001B[38;5;241m=\u001B[39mn_folds,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     42\u001B[0m                 verbose\u001B[38;5;241m=\u001B[39mverbosity,\n\u001B[0;32m     43\u001B[0m                 random_state\u001B[38;5;241m=\u001B[39mseed)\n\u001B[1;32m---> 45\u001B[0m grid_model \u001B[38;5;241m=\u001B[39m \u001B[43mcv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m(X, y)\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m grid_model\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'HalvingGridSearchCV' object has no attribute 'fit_transform'"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(solver='liblinear',\n",
    "                           random_state=seed)\n",
    "\n",
    "model_name = 'logreg_nlp'\n",
    "\n",
    "parameters = {f'{model_name}__C': np.logspace(-1, 0, 15)}\n",
    "\n",
    "logreg_nlp_gridCV = create_nlp_pipeline(train_set = [X_train_text, y_train],\n",
    "                                        preproc=create_nlp_preproc(post_feature='request_text_edit_aware',\n",
    "                                                                   title_feature='request_title',\n",
    "                                                                   scaler=Normalizer()),\n",
    "                                        estimator = model,\n",
    "                                        model_name = model_name,\n",
    "                                        grid_strat=HalvingGridSearchCV,\n",
    "                                        hyperparameters = parameters,\n",
    "                                        n_folds = 4,\n",
    "                                        eval_metric='precision',\n",
    "                                        verbosity=0)\n",
    "\n",
    "print()\n",
    "print_pipe_results(train_set = [X_train_text, y_train],\n",
    "                   val_set = [X_val_text, y_val],\n",
    "                   model = logreg_nlp_gridCV)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Gaussian Naive Bayes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In samples resutls:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.76      1.00      0.87      2741\n",
      "        True       1.00      0.04      0.09       895\n",
      "\n",
      "    accuracy                           0.76      3636\n",
      "   macro avg       0.88      0.52      0.48      3636\n",
      "weighted avg       0.82      0.76      0.67      3636\n",
      "\n",
      "\n",
      "Out-of-samples resutls:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.76      1.00      0.86       153\n",
      "        True       0.00      0.00      0.00        49\n",
      "\n",
      "    accuracy                           0.76       202\n",
      "   macro avg       0.38      0.50      0.43       202\n",
      "weighted avg       0.57      0.76      0.65       202\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "      PredNeg  PredPos\n",
      "Neg  0.757426      0.0\n",
      "Pos  0.242574      0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "\n",
    "model_name = 'gaus_nb'\n",
    "\n",
    "parameters = {}\n",
    "\n",
    "gaussnb_nlp_gridCV = create_nlp_pipeline(train_set = [X_train_text, y_train],\n",
    "                                         preproc=create_nlp_preproc(post_feature='request_text_edit_aware',\n",
    "                                                                    title_feature='request_title',\n",
    "                                                                    scaler=None,\n",
    "                                                                    reducter=None),\n",
    "                                         estimator = model,\n",
    "                                         model_name = model_name,\n",
    "                                         grid_strat=HalvingGridSearchCV,\n",
    "                                         hyperparameters = parameters,\n",
    "                                         n_folds = 4,\n",
    "                                         eval_metric='precision',\n",
    "                                         verbosity=0)\n",
    "\n",
    "print()\n",
    "print_pipe_results(train_set = [X_train_text, y_train],\n",
    "                   val_set = [X_val_text, y_val],\n",
    "                   model = gaussnb_nlp_gridCV)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Random Forest Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In samples resutls:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.76      1.00      0.86      2741\n",
      "        True       1.00      0.02      0.05       895\n",
      "\n",
      "    accuracy                           0.76      3636\n",
      "   macro avg       0.88      0.51      0.45      3636\n",
      "weighted avg       0.82      0.76      0.66      3636\n",
      "\n",
      "\n",
      "Out-of-samples resutls:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.76      1.00      0.86       153\n",
      "        True       0.00      0.00      0.00        49\n",
      "\n",
      "    accuracy                           0.76       202\n",
      "   macro avg       0.38      0.50      0.43       202\n",
      "weighted avg       0.57      0.76      0.65       202\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "      PredNeg  PredPos\n",
      "Neg  0.757426      0.0\n",
      "Pos  0.242574      0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=seed, n_jobs=10)\n",
    "\n",
    "model_name = 'rf'\n",
    "\n",
    "parameters = {f'{model_name}__n_estimators': np.linspace(50, 500, 4).astype(int),\n",
    "              f'{model_name}__max_depth': np.linspace(8, 25, 4).astype(int)}\n",
    "\n",
    "rf_nlp_gridCV = create_nlp_pipeline(train_set = [X_train_text, y_train],\n",
    "                                    preproc=create_nlp_preproc(post_feature='request_text_edit_aware',\n",
    "                                                               title_feature='request_title'),\n",
    "                                    estimator = model,\n",
    "                                    model_name = model_name,\n",
    "                                    grid_strat=HalvingGridSearchCV,\n",
    "                                    hyperparameters = parameters,\n",
    "                                    n_folds = 4,\n",
    "                                    eval_metric='precision',\n",
    "                                    verbosity=0)\n",
    "\n",
    "print()\n",
    "print_pipe_results(train_set = [X_train_text, y_train],\n",
    "                   val_set = [X_val_text, y_val],\n",
    "                   model = rf_nlp_gridCV)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### XGBoost Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 4\n",
      "n_required_iterations: 4\n",
      "n_possible_iterations: 4\n",
      "min_resources_: 134\n",
      "max_resources_: 3636\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 72\n",
      "n_resources: 134\n",
      "Fitting 4 folds for each of 72 candidates, totalling 288 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 24\n",
      "n_resources: 402\n",
      "Fitting 4 folds for each of 24 candidates, totalling 96 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 8\n",
      "n_resources: 1206\n",
      "Fitting 4 folds for each of 8 candidates, totalling 32 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 3\n",
      "n_resources: 3618\n",
      "Fitting 4 folds for each of 3 candidates, totalling 12 fits\n",
      "\n",
      "In samples resutls:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.78      0.39      0.52      2741\n",
      "        True       0.26      0.66      0.37       895\n",
      "\n",
      "    accuracy                           0.45      3636\n",
      "   macro avg       0.52      0.52      0.44      3636\n",
      "weighted avg       0.65      0.45      0.48      3636\n",
      "\n",
      "\n",
      "Out-of-samples resutls:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.78      0.39      0.52       153\n",
      "        True       0.25      0.65      0.37        49\n",
      "\n",
      "    accuracy                           0.45       202\n",
      "   macro avg       0.52      0.52      0.44       202\n",
      "weighted avg       0.65      0.45      0.48       202\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "      PredNeg   PredPos\n",
      "Neg  0.292079  0.465347\n",
      "Pos  0.084158  0.158416\n"
     ]
    }
   ],
   "source": [
    "model = xgb.XGBClassifier(random_state=seed,\n",
    "                          objective='binary:logistic',\n",
    "                          eval_metric=precision_score,\n",
    "                          tree_method='gpu_hist')\n",
    "\n",
    "model_name = 'xgb_cl'\n",
    "\n",
    "parameters = {f'{model_name}__n_estimators': np.linspace(20, 500, 3).astype(int),\n",
    "              f'{model_name}__learning_rate': np.logspace(-1, 0, 2),\n",
    "              f'{model_name}__max_depth': np.linspace(2, 9, 3).astype(int),\n",
    "              f'{model_name}__booster': ['gbtree'],\n",
    "              f'{model_name}__colsample_bytree': np.logspace(-0.7, 0, 2),\n",
    "              f'{model_name}__subsample': np.logspace(-0.7, 0, 2)\n",
    "             }\n",
    "\n",
    "xgb_nlp_gridCV = create_nlp_pipeline(train_set = [X_train_text, y_train],\n",
    "                                     preproc=create_nlp_preproc(post_feature='request_text_edit_aware',\n",
    "                                                                title_feature='request_title'),\n",
    "                                     estimator = model,\n",
    "                                     model_name = model_name,\n",
    "                                     grid_strat=HalvingGridSearchCV,\n",
    "                                     hyperparameters = parameters,\n",
    "                                     n_folds = 4,\n",
    "                                     eval_metric='precision', # f1 is bad -> 0.47 precision / 0.46 recall\n",
    "                                     verbosity=1)\n",
    "\n",
    "print()\n",
    "print_pipe_results(train_set = [X_train_text, y_train],\n",
    "                   val_set = [X_val_text, y_val],\n",
    "                   model = xgb_nlp_gridCV)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Conclusion\n",
    "\n",
    "After model comparison and light hypertunings, results are much more encouraging.\n",
    "Let's conclude with a final part incorporating all data types."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## One model to rule them all.\n",
    "\n",
    "### Feature engineering\n",
    "\n",
    "Let's add previous feature creation process, as well as a numeric one based on the text length."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [],
   "source": [
    "def add_text_length(df):\n",
    "    post_feat = 'request_text_edit_aware'\n",
    "    df[[f'{post_feat}_len']] = df.copy()[[post_feat]].apply(len)\n",
    "\n",
    "    return df\n",
    "\n",
    "X_train_lotr = add_text_length(X_train)\n",
    "X_val_lotr = add_text_length(X_val)\n",
    "\n",
    "X_train_lotr = add_relative_votes(X_train_lotr)\n",
    "X_val_lotr = add_relative_votes(X_val_lotr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "outputs": [],
   "source": [
    "def create_lotr_preproc(numerical_features, post_feature, title_feature, scaler_num=None, scaler_text=None, reducter=None):\n",
    "    # Non-textual data\n",
    "    steps = [('scaler_method', scaler_num)]\n",
    "    scaler_transformer = Pipeline(steps)\n",
    "\n",
    "    # Word vectorizers\n",
    "    # Textual data\n",
    "    text_vectorizer = CountVectorizer(lowercase=True,\n",
    "                                      tokenizer=word_tokenize,\n",
    "                                      token_pattern=None,\n",
    "                                      stop_words=list(stopwords_en),\n",
    "                                      min_df=3,\n",
    "                                      ngram_range=(1, 2))\n",
    "    text_tfidf = TfidfTransformer()\n",
    "    steps = [('text_count', text_vectorizer), ('text_tfidf', text_tfidf), ('text_reduc', reducter), ('text_scaler', scaler_text)]\n",
    "    text_pipe = Pipeline(steps)\n",
    "\n",
    "    # Preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num_transformer', scaler_transformer, numerical_features),\n",
    "            ('wordvec_post_transformer', text_pipe, post_feature),\n",
    "            ('wordvec_title_transformer', text_pipe, title_feature),\n",
    "        ], remainder='drop')\n",
    "\n",
    "    steps = [('preproc', preprocessor)]\n",
    "    preproc_pipe = Pipeline(steps)\n",
    "\n",
    "    return preproc_pipe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In samples resutls:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.81      1.00      0.89      2741\n",
      "        True       1.00      0.28      0.44       895\n",
      "\n",
      "    accuracy                           0.82      3636\n",
      "   macro avg       0.90      0.64      0.67      3636\n",
      "weighted avg       0.86      0.82      0.78      3636\n",
      "\n",
      "\n",
      "Out-of-samples resutls:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.76      0.97      0.85       153\n",
      "        True       0.33      0.04      0.07        49\n",
      "\n",
      "    accuracy                           0.75       202\n",
      "   macro avg       0.55      0.51      0.46       202\n",
      "weighted avg       0.66      0.75      0.66       202\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "      PredNeg   PredPos\n",
      "Neg  0.737624  0.019802\n",
      "Pos  0.232673  0.009901\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(solver='liblinear',\n",
    "                           random_state=seed)\n",
    "\n",
    "model_name = 'logreg_lotr'\n",
    "\n",
    "parameters = {f'{model_name}__C': np.logspace(-1, 0, 40)}\n",
    "\n",
    "logreg_lotr_gridCV = create_nlp_pipeline(train_set = [X_train_lotr, y_train],\n",
    "                                         preproc=create_lotr_preproc(numerical_features=X_train_lotr.select_dtypes(exclude=[\"object\"]).columns,\n",
    "                                                                     post_feature='request_text_edit_aware',\n",
    "                                                                     title_feature='request_title',\n",
    "                                                                     scaler_num=Normalizer()),\n",
    "                                         estimator = model,\n",
    "                                         model_name = model_name,\n",
    "                                         grid_strat=HalvingGridSearchCV,\n",
    "                                         hyperparameters = parameters,\n",
    "                                         n_folds = 3,\n",
    "                                         eval_metric='recall',\n",
    "                                         verbosity=0)\n",
    "\n",
    "print()\n",
    "print_pipe_results(train_set = [X_train_lotr, y_train],\n",
    "                   val_set = [X_val_lotr, y_val],\n",
    "                   model = logreg_lotr_gridCV)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=3.8881551803080874, random_state=101, solver='liblinear')\n"
     ]
    }
   ],
   "source": [
    "print(logreg_lotr_gridCV.best_estimator_['logreg_lotr'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This time the logistic regression is quite better both in precision and in accuracy, but still have a margin of improvements in recall."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Gaussian Naive Bayes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 1\n",
      "n_required_iterations: 1\n",
      "n_possible_iterations: 1\n",
      "min_resources_: 3636\n",
      "max_resources_: 3636\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 1\n",
      "n_resources: 3636\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "\n",
      "In samples resutls:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.76      1.00      0.86      2741\n",
      "        True       1.00      0.02      0.04       895\n",
      "\n",
      "    accuracy                           0.76      3636\n",
      "   macro avg       0.88      0.51      0.45      3636\n",
      "weighted avg       0.82      0.76      0.66      3636\n",
      "\n",
      "\n",
      "Out-of-samples resutls:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.76      1.00      0.86       153\n",
      "        True       0.00      0.00      0.00        49\n",
      "\n",
      "    accuracy                           0.76       202\n",
      "   macro avg       0.38      0.50      0.43       202\n",
      "weighted avg       0.57      0.76      0.65       202\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "      PredNeg  PredPos\n",
      "Neg  0.757426      0.0\n",
      "Pos  0.242574      0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "\n",
    "model_name = 'gausnb_lotr'\n",
    "\n",
    "parameters = {}\n",
    "\n",
    "gaussnb_lotr_gridCV = create_nlp_pipeline(train_set = [X_train_lotr, y_train],\n",
    "                                         preproc=create_lotr_preproc(numerical_features=X_train_lotr.select_dtypes(exclude=[\"object\"]).columns,\n",
    "                                                                     post_feature='request_text_edit_aware',\n",
    "                                                                     title_feature='request_title',\n",
    "                                                                     scaler_num=MinMaxScaler()),\n",
    "                                         estimator = model,\n",
    "                                         model_name = model_name,\n",
    "                                         grid_strat=HalvingGridSearchCV,\n",
    "                                         hyperparameters = parameters,\n",
    "                                         n_folds = 3,\n",
    "                                         eval_metric='precision',\n",
    "                                         verbosity=1)\n",
    "\n",
    "print()\n",
    "print_pipe_results(train_set = [X_train_lotr, y_train],\n",
    "                   val_set = [X_val_lotr, y_val],\n",
    "                   model = gaussnb_lotr_gridCV)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Random Forest Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In samples resutls:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.77      1.00      0.87      2741\n",
      "        True       1.00      0.08      0.14       895\n",
      "\n",
      "    accuracy                           0.77      3636\n",
      "   macro avg       0.88      0.54      0.51      3636\n",
      "weighted avg       0.83      0.77      0.69      3636\n",
      "\n",
      "\n",
      "Out-of-samples resutls:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.76      1.00      0.86       153\n",
      "        True       1.00      0.02      0.04        49\n",
      "\n",
      "    accuracy                           0.76       202\n",
      "   macro avg       0.88      0.51      0.45       202\n",
      "weighted avg       0.82      0.76      0.66       202\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "      PredNeg  PredPos\n",
      "Neg  0.757426  0.00000\n",
      "Pos  0.237624  0.00495\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=seed, n_jobs=10)\n",
    "\n",
    "model_name = 'rf_lotr'\n",
    "\n",
    "parameters = {f'{model_name}__n_estimators': np.linspace(10, 350, 5).astype(int),\n",
    "              f'{model_name}__max_depth': np.linspace(2, 20, 7).astype(int)}\n",
    "\n",
    "rf_lotr_gridCV = create_nlp_pipeline(train_set = [X_train_lotr, y_train],\n",
    "                                    preproc=create_lotr_preproc(numerical_features=X_train_lotr.select_dtypes(exclude=[\"object\"]).columns,\n",
    "                                                                post_feature='request_text_edit_aware',\n",
    "                                                                title_feature='request_title'),\n",
    "                                    estimator = model,\n",
    "                                    model_name = model_name,\n",
    "                                    grid_strat=HalvingGridSearchCV,\n",
    "                                    hyperparameters = parameters,\n",
    "                                    n_folds = 3,\n",
    "                                    eval_metric='precision',\n",
    "                                    verbosity=0)\n",
    "\n",
    "print()\n",
    "print_pipe_results(train_set = [X_train_lotr, y_train],\n",
    "                   val_set = [X_val_lotr, y_val],\n",
    "                   model = rf_lotr_gridCV)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### XGBoost Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 432 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n288 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 402, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 360, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 894, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 446, in fit_transform\n    return last_step.fit_transform(Xt, y, **fit_params_last_step)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 726, in fit_transform\n    result = self._fit_transform(X, y, _fit_transform_one)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 657, in _fit_transform\n    return Parallel(n_jobs=self.n_jobs)(\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\joblib\\parallel.py\", line 1088, in __call__\n    while self.dispatch_one_batch(iterator):\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n    self._dispatch(tasks)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n    job = self._backend.apply_async(batch, callback=cb)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n    result = ImmediateResult(func)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n    self.results = batch()\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 117, in __call__\n    return self.function(*args, **kwargs)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 894, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 438, in fit_transform\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 360, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 894, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py\", line 237, in fit_transform\n    raise ValueError(\nValueError: n_components(20) must be <= n_features(8).\n\n--------------------------------------------------------------------------------\n144 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 402, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 360, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 894, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 446, in fit_transform\n    return last_step.fit_transform(Xt, y, **fit_params_last_step)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 726, in fit_transform\n    result = self._fit_transform(X, y, _fit_transform_one)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 657, in _fit_transform\n    return Parallel(n_jobs=self.n_jobs)(\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\joblib\\parallel.py\", line 1088, in __call__\n    while self.dispatch_one_batch(iterator):\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n    self._dispatch(tasks)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n    job = self._backend.apply_async(batch, callback=cb)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n    result = ImmediateResult(func)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n    self.results = batch()\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 117, in __call__\n    return self.function(*args, **kwargs)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 894, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 438, in fit_transform\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 360, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 894, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py\", line 237, in fit_transform\n    raise ValueError(\nValueError: n_components(20) must be <= n_features(4).\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[242], line 16\u001B[0m\n\u001B[0;32m      6\u001B[0m model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mxgb_cl_lotr\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      8\u001B[0m parameters \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m__n_estimators\u001B[39m\u001B[38;5;124m'\u001B[39m: np\u001B[38;5;241m.\u001B[39mlinspace(\u001B[38;5;241m20\u001B[39m, \u001B[38;5;241m300\u001B[39m, \u001B[38;5;241m3\u001B[39m)\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mint\u001B[39m),\n\u001B[0;32m      9\u001B[0m               \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m__learning_rate\u001B[39m\u001B[38;5;124m'\u001B[39m: np\u001B[38;5;241m.\u001B[39mlogspace(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m4\u001B[39m),\n\u001B[0;32m     10\u001B[0m               \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m__max_depth\u001B[39m\u001B[38;5;124m'\u001B[39m: np\u001B[38;5;241m.\u001B[39mlinspace(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m12\u001B[39m, \u001B[38;5;241m3\u001B[39m)\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mint\u001B[39m),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     13\u001B[0m               \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m__subsample\u001B[39m\u001B[38;5;124m'\u001B[39m: np\u001B[38;5;241m.\u001B[39mlogspace(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m0.7\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m     14\u001B[0m              }\n\u001B[1;32m---> 16\u001B[0m xgb_lotr_gridCV \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_nlp_pipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_set\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mX_train_lotr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[43m                                    \u001B[49m\u001B[43mpreproc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcreate_lotr_preproc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnumerical_features\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX_train_lotr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect_dtypes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mobject\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     18\u001B[0m \u001B[43m                                                                \u001B[49m\u001B[43mpost_feature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrequest_text_edit_aware\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     19\u001B[0m \u001B[43m                                                                \u001B[49m\u001B[43mtitle_feature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrequest_title\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     20\u001B[0m \u001B[43m                                                                \u001B[49m\u001B[43mreducter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mTruncatedSVD\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_components\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     21\u001B[0m \u001B[43m                                    \u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     22\u001B[0m \u001B[43m                                    \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     23\u001B[0m \u001B[43m                                    \u001B[49m\u001B[43mgrid_strat\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mHalvingGridSearchCV\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     24\u001B[0m \u001B[43m                                    \u001B[49m\u001B[43mhyperparameters\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     25\u001B[0m \u001B[43m                                    \u001B[49m\u001B[43mn_folds\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     26\u001B[0m \u001B[43m                                    \u001B[49m\u001B[43meval_metric\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mf1\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     27\u001B[0m \u001B[43m                                    \u001B[49m\u001B[43mverbosity\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28mprint\u001B[39m()\n\u001B[0;32m     30\u001B[0m print_pipe_results(train_set \u001B[38;5;241m=\u001B[39m [X_train_lotr, y_train],\n\u001B[0;32m     31\u001B[0m                    val_set \u001B[38;5;241m=\u001B[39m [X_val_lotr, y_val],\n\u001B[0;32m     32\u001B[0m                    model \u001B[38;5;241m=\u001B[39m xgb_lotr_gridCV)\n",
      "Cell \u001B[1;32mIn[240], line 45\u001B[0m, in \u001B[0;36mcreate_nlp_pipeline\u001B[1;34m(train_set, preproc, estimator, model_name, grid_strat, hyperparameters, n_folds, eval_metric, verbosity, n_jobs)\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;66;03m# Grid Search\u001B[39;00m\n\u001B[0;32m     37\u001B[0m cv \u001B[38;5;241m=\u001B[39m grid_strat(model_pipe,\n\u001B[0;32m     38\u001B[0m                 param_grid\u001B[38;5;241m=\u001B[39mhyperparameters,\n\u001B[0;32m     39\u001B[0m                 cv\u001B[38;5;241m=\u001B[39mn_folds,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     42\u001B[0m                 verbose\u001B[38;5;241m=\u001B[39mverbosity,\n\u001B[0;32m     43\u001B[0m                 random_state\u001B[38;5;241m=\u001B[39mseed)\n\u001B[1;32m---> 45\u001B[0m grid_model \u001B[38;5;241m=\u001B[39m \u001B[43mcv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m grid_model\n",
      "File \u001B[1;32m~\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search_successive_halving.py:273\u001B[0m, in \u001B[0;36mBaseSuccessiveHalving.fit\u001B[1;34m(self, X, y, groups, **fit_params)\u001B[0m\n\u001B[0;32m    265\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_input_parameters(\n\u001B[0;32m    266\u001B[0m     X\u001B[38;5;241m=\u001B[39mX,\n\u001B[0;32m    267\u001B[0m     y\u001B[38;5;241m=\u001B[39my,\n\u001B[0;32m    268\u001B[0m     groups\u001B[38;5;241m=\u001B[39mgroups,\n\u001B[0;32m    269\u001B[0m )\n\u001B[0;32m    271\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_samples_orig \u001B[38;5;241m=\u001B[39m _num_samples(X)\n\u001B[1;32m--> 273\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mfit(X, y\u001B[38;5;241m=\u001B[39my, groups\u001B[38;5;241m=\u001B[39mgroups, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n\u001B[0;32m    275\u001B[0m \u001B[38;5;66;03m# Set best_score_: BaseSearchCV does not set it, as refit is a callable\u001B[39;00m\n\u001B[0;32m    276\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbest_score_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcv_results_[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmean_test_score\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbest_index_]\n",
      "File \u001B[1;32m~\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:875\u001B[0m, in \u001B[0;36mBaseSearchCV.fit\u001B[1;34m(self, X, y, groups, **fit_params)\u001B[0m\n\u001B[0;32m    869\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_results(\n\u001B[0;32m    870\u001B[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[0;32m    871\u001B[0m     )\n\u001B[0;32m    873\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[1;32m--> 875\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    877\u001B[0m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[0;32m    878\u001B[0m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[0;32m    879\u001B[0m first_test_score \u001B[38;5;241m=\u001B[39m all_out[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_scores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search_successive_halving.py:378\u001B[0m, in \u001B[0;36mBaseSuccessiveHalving._run_search\u001B[1;34m(self, evaluate_candidates)\u001B[0m\n\u001B[0;32m    371\u001B[0m     cv \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checked_cv_orig\n\u001B[0;32m    373\u001B[0m more_results \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    374\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miter\u001B[39m\u001B[38;5;124m\"\u001B[39m: [itr] \u001B[38;5;241m*\u001B[39m n_candidates,\n\u001B[0;32m    375\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn_resources\u001B[39m\u001B[38;5;124m\"\u001B[39m: [n_resources] \u001B[38;5;241m*\u001B[39m n_candidates,\n\u001B[0;32m    376\u001B[0m }\n\u001B[1;32m--> 378\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    379\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcandidate_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmore_results\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmore_results\u001B[49m\n\u001B[0;32m    380\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    382\u001B[0m n_candidates_to_keep \u001B[38;5;241m=\u001B[39m ceil(n_candidates \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfactor)\n\u001B[0;32m    383\u001B[0m candidate_params \u001B[38;5;241m=\u001B[39m _top_k(results, n_candidates_to_keep, itr)\n",
      "File \u001B[1;32m~\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:852\u001B[0m, in \u001B[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[1;34m(candidate_params, cv, more_results)\u001B[0m\n\u001B[0;32m    845\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m!=\u001B[39m n_candidates \u001B[38;5;241m*\u001B[39m n_splits:\n\u001B[0;32m    846\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    847\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcv.split and cv.get_n_splits returned \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    848\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minconsistent results. Expected \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    849\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msplits, got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(n_splits, \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m n_candidates)\n\u001B[0;32m    850\u001B[0m     )\n\u001B[1;32m--> 852\u001B[0m \u001B[43m_warn_or_raise_about_fit_failures\u001B[49m\u001B[43m(\u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43merror_score\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    854\u001B[0m \u001B[38;5;66;03m# For callable self.scoring, the return type is only know after\u001B[39;00m\n\u001B[0;32m    855\u001B[0m \u001B[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001B[39;00m\n\u001B[0;32m    856\u001B[0m \u001B[38;5;66;03m# can now be inserted with the correct key. The type checking\u001B[39;00m\n\u001B[0;32m    857\u001B[0m \u001B[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001B[39;00m\n\u001B[0;32m    858\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m callable(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscoring):\n",
      "File \u001B[1;32m~\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:367\u001B[0m, in \u001B[0;36m_warn_or_raise_about_fit_failures\u001B[1;34m(results, error_score)\u001B[0m\n\u001B[0;32m    360\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m num_failed_fits \u001B[38;5;241m==\u001B[39m num_fits:\n\u001B[0;32m    361\u001B[0m     all_fits_failed_message \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    362\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mAll the \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_fits\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m fits failed.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    363\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIt is very likely that your model is misconfigured.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    364\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou can try to debug the error by setting error_score=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mraise\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    365\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBelow are more details about the failures:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mfit_errors_summary\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    366\u001B[0m     )\n\u001B[1;32m--> 367\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(all_fits_failed_message)\n\u001B[0;32m    369\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    370\u001B[0m     some_fits_failed_message \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    371\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mnum_failed_fits\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m fits failed out of a total of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_fits\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    372\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe score on these train-test partitions for these parameters\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    376\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBelow are more details about the failures:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mfit_errors_summary\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    377\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: \nAll the 432 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n288 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 402, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 360, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 894, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 446, in fit_transform\n    return last_step.fit_transform(Xt, y, **fit_params_last_step)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 726, in fit_transform\n    result = self._fit_transform(X, y, _fit_transform_one)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 657, in _fit_transform\n    return Parallel(n_jobs=self.n_jobs)(\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\joblib\\parallel.py\", line 1088, in __call__\n    while self.dispatch_one_batch(iterator):\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n    self._dispatch(tasks)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n    job = self._backend.apply_async(batch, callback=cb)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n    result = ImmediateResult(func)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n    self.results = batch()\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 117, in __call__\n    return self.function(*args, **kwargs)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 894, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 438, in fit_transform\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 360, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 894, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py\", line 237, in fit_transform\n    raise ValueError(\nValueError: n_components(20) must be <= n_features(8).\n\n--------------------------------------------------------------------------------\n144 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 402, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 360, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 894, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 446, in fit_transform\n    return last_step.fit_transform(Xt, y, **fit_params_last_step)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 726, in fit_transform\n    result = self._fit_transform(X, y, _fit_transform_one)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 657, in _fit_transform\n    return Parallel(n_jobs=self.n_jobs)(\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\joblib\\parallel.py\", line 1088, in __call__\n    while self.dispatch_one_batch(iterator):\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n    self._dispatch(tasks)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n    job = self._backend.apply_async(batch, callback=cb)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n    result = ImmediateResult(func)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n    self.results = batch()\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 117, in __call__\n    return self.function(*args, **kwargs)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 894, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 438, in fit_transform\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 360, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 894, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"C:\\Users\\Akim van Eersel\\Python_Projects\\kaggle\\Random Acts of Pizza - CybelAngel\\venv\\lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py\", line 237, in fit_transform\n    raise ValueError(\nValueError: n_components(20) must be <= n_features(4).\n"
     ]
    }
   ],
   "source": [
    "model = xgb.XGBClassifier(random_state=seed,\n",
    "                          objective='binary:logistic',\n",
    "                          eval_metric=precision_score,\n",
    "                          tree_method='gpu_hist')\n",
    "\n",
    "model_name = 'xgb_cl_lotr'\n",
    "\n",
    "parameters = {f'{model_name}__n_estimators': np.linspace(20, 300, 3).astype(int),\n",
    "              f'{model_name}__learning_rate': np.logspace(-1, 0, 4),\n",
    "              f'{model_name}__max_depth': np.linspace(2, 12, 3).astype(int),\n",
    "              f'{model_name}__booster': ['gbtree'],\n",
    "              f'{model_name}__colsample_bytree': np.logspace(-0.7, 0, 2),\n",
    "              f'{model_name}__subsample': np.logspace(-0.7, 0, 2)\n",
    "             }\n",
    "\n",
    "xgb_lotr_gridCV = create_nlp_pipeline(train_set = [X_train_lotr, y_train],\n",
    "                                    preproc=create_lotr_preproc(numerical_features=X_train_lotr.select_dtypes(exclude=[\"object\"]).columns,\n",
    "                                                                post_feature='request_text_edit_aware',\n",
    "                                                                title_feature='request_title',\n",
    "                                                                reducter=NMF(random_state=seed, n_components=20)),\n",
    "                                    estimator = model,\n",
    "                                    model_name = model_name,\n",
    "                                    grid_strat=HalvingGridSearchCV,\n",
    "                                    hyperparameters = parameters,\n",
    "                                    n_folds = 3,\n",
    "                                    eval_metric='f1',\n",
    "                                    verbosity=0)\n",
    "\n",
    "print()\n",
    "print_pipe_results(train_set = [X_train_lotr, y_train],\n",
    "                   val_set = [X_val_lotr, y_val],\n",
    "                   model = xgb_lotr_gridCV)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "outputs": [
    {
     "data": {
      "text/plain": "<3636x12055 sparse matrix of type '<class 'numpy.float64'>'\n\twith 211613 stored elements in Compressed Sparse Row format>"
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pipeline([('preproc', create_lotr_preproc(numerical_features=X_train_lotr.select_dtypes(exclude=[\"object\"]).columns,\n",
    "                                          post_feature='request_text_edit_aware',\n",
    "                                          title_feature='request_title'))]).fit_transform(X_train_lotr)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
